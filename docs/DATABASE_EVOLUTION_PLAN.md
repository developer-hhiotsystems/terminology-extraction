# Database Evolution Plan
## From SQLite to Production-Ready Architecture

**Date:** 2025-10-19
**Database Expert:** Database Architecture Specialist
**Context:** ~2,700 cleaned terms, 98% extraction quality, relationship extraction planned for Month 2

---

## Executive Summary

### Current State: SQLite (Good Foundation)
- **Database:** SQLite 3.x with SQLAlchemy 2.0
- **Data Quality:** 98% (excellent improvement from 40-45% baseline)
- **Size:** ~2,700 terms across 3 documents
- **Performance:** Adequate for current scale
- **Schema:** Hybrid relational-document model with JSON fields

### Recommended Evolution Path

```
Month 1-2 (NOW):
  âœ… Keep SQLite
  âœ… Normalize schema (fix JSON issues)
  âœ… Add relationship tables
  âœ… Implement FTS5 full-text search

Month 3-4 (Scale Phase):
  âš ï¸ Migrate to PostgreSQL
  âš ï¸ Add advanced indexing
  âš ï¸ Optimize for 10K+ terms

Month 5+ (Optional):
  ğŸ”µ Consider Neo4j (ONLY if relationships become core feature)
  ğŸ”µ Evaluate PostgreSQL + AGE (graph extension)
```

### Key Decision: **PostgreSQL First, Neo4j Later (If Ever)**

**Why PostgreSQL?**
- Single database (no sync complexity)
- Superior JSON querying vs SQLite
- Full-text search with tsvector
- Better concurrency for multi-user
- Foundation for future scale
- Optional graph extensions (Apache AGE)

**Why NOT Neo4j Yet?**
- No critical features blocked
- Dual-database sync adds complexity
- Relationships not yet extracted (Month 2 planned)
- PostgreSQL can handle relationship queries adequately
- Can add Neo4j later if truly needed

---

## 1. Current Schema Assessment

### 1.1 Schema Strengths âœ…

**Good Design Patterns:**

1. **Proper Foreign Keys**
   ```sql
   -- Documents â†’ Entries (many-to-many)
   term_document_references
     glossary_entry_id â†’ glossary_entries.id
     document_id â†’ uploaded_documents.id
   ```

2. **Appropriate Indexing**
   - Composite index: `(term, language)`
   - Individual indexes: `source`, `validation_status`, `sync_status`
   - Foreign key indexes for joins

3. **Data Integrity Constraints**
   ```sql
   -- Language validation
   CHECK (language IN ('de', 'en'))

   -- Unique terms per source
   UNIQUE (term, language, source)

   -- Validation status check
   CHECK (validation_status IN ('pending', 'validated', 'rejected'))
   ```

4. **Audit Trail**
   - `creation_date`, `updated_at` on all tables
   - `sync_logs` table for troubleshooting
   - `processing_metadata` JSON for extraction details

5. **Bilingual Support**
   - `DocumentType` has `label_en` and `label_de`
   - Language field with constraints

### 1.2 Critical Schema Issues âŒ

#### Issue #1: No Referential Integrity for JSON Fields

**Problem:**
```python
# definitions is JSON array with embedded references
definitions = Column(JSON, nullable=False)
# Contains: [{"text": "...", "source_doc_id": 123, "is_primary": true}]

# âŒ source_doc_id has NO foreign key constraint
# âŒ Can reference non-existent documents
# âŒ Orphaned references on document deletion
```

**Impact:** Data inconsistency, orphaned references, no cascade delete

**Solution:** Normalize to separate table (see Section 4.1)

---

#### Issue #2: Poor JSON Queryability

**Problem:**
```python
# Current: Cannot efficiently query by definition attributes
db.query(GlossaryEntry).filter(
    # âŒ SQLite has limited JSON support
    GlossaryEntry.definitions.contains({"is_primary": True})
)

# âŒ Cannot use indexes on JSON fields in SQLite
# âŒ Cannot filter by source_doc_id in definitions
```

**Impact:** Slow queries, cannot implement "find all definitions from document X"

**Solution:**
- Short-term: Normalize to `term_definitions` table
- Medium-term: Migrate to PostgreSQL for better JSON support

---

#### Issue #3: No Full-Text Search

**Problem:**
```python
# Current search (only matches term field)
search_query = db.query(GlossaryEntry).filter(
    GlossaryEntry.term.ilike(f"%{query}%")  # âŒ Slow, no relevance ranking
)

# âŒ Does NOT search definition text
# âŒ No fuzzy matching
# âŒ No relevance scoring
```

**Impact:** Users cannot find terms by definition content, slow searches on large datasets

**Solution:** SQLite FTS5 (short-term) or PostgreSQL tsvector (medium-term)

---

#### Issue #4: No Relationship Modeling

**Problem:**
```python
# âŒ No tables for term-to-term relationships
# - SYNONYM_OF: "Bioreactor" â†” "Bioreaktor"
# - PART_OF: "Safety Valve" â†’ "Valve"
# - RELATED_TO: "Mixing" â†” "Impeller"
# - OPPOSITE_OF: "Inlet" â†” "Outlet"
# - ABBREVIATION_OF: "PID" â†’ "Proportional Integral Derivative"
```

**Impact:**
- Cannot implement synonym browsing
- Cannot show hierarchies
- Cannot suggest related terms
- This is the main argument FOR Neo4j

**Solution:** Add `term_relationships` table (see Section 4.2)

---

#### Issue #5: Missing Data Quality Constraints

**Problem:**
```python
# âŒ No constraints to prevent garbage data
term = Column(String(255), nullable=False)  # Allows any string!

# Examples of bad data that passed validation:
# - Empty strings
# - Pure whitespace
# - Single characters
# - OCR artifacts ("Tthhee", "Oonn")
```

**Impact:** 98% quality is good, but 2% (54 terms) are still bad

**Solution:** Database-level constraints + improved application validation

---

## 2. PostgreSQL Migration Analysis

### 2.1 Why PostgreSQL? âœ…

#### Benefit #1: Superior JSON Support

**PostgreSQL:**
```sql
-- Fast JSON queries with GIN indexes
CREATE INDEX idx_definitions_source
  ON term_definitions USING GIN ((definition_data->'source_doc_id'));

-- Query specific JSON attributes efficiently
SELECT * FROM glossary_entries
WHERE definitions @> '[{"is_primary": true}]'::jsonb;

-- Extract JSON values with operators
SELECT term, definitions->0->>'text' AS primary_def
FROM glossary_entries;
```

**SQLite:**
```sql
-- âŒ Limited JSON functions
-- âŒ No JSON indexes
-- âŒ Slower JSON queries
```

**Verdict:** PostgreSQL 10x better for JSON operations

---

#### Benefit #2: Full-Text Search (tsvector)

**PostgreSQL:**
```sql
-- Create FTS column with automatic updates
ALTER TABLE glossary_entries
  ADD COLUMN search_vector tsvector
  GENERATED ALWAYS AS (
    setweight(to_tsvector('english', coalesce(term, '')), 'A') ||
    setweight(to_tsvector('german', coalesce(definition_text, '')), 'B')
  ) STORED;

-- GIN index for fast FTS
CREATE INDEX idx_fts_search ON glossary_entries USING GIN (search_vector);

-- Rank search results by relevance
SELECT term, ts_rank(search_vector, query) AS rank
FROM glossary_entries, to_tsquery('reactor & bioreactor') AS query
WHERE search_vector @@ query
ORDER BY rank DESC;
```

**SQLite FTS5:**
```sql
-- External virtual table (separate from main table)
CREATE VIRTUAL TABLE glossary_fts USING fts5(term, definition_text);

-- âŒ Requires manual sync with main table
-- âŒ No multi-language stemming
-- âš ï¸ Good but less powerful than PostgreSQL
```

**Verdict:** PostgreSQL offers richer FTS capabilities

---

#### Benefit #3: Concurrency & Multi-User Support

| Feature | SQLite | PostgreSQL |
|---------|--------|------------|
| **Concurrent Readers** | Unlimited | Unlimited |
| **Concurrent Writers** | âŒ 1 (locks entire DB) | âœ… Unlimited (MVCC) |
| **Write Performance** | Slow under contention | Fast (row-level locking) |
| **Multi-User Safe** | âš ï¸ Limited | âœ… Yes |

**Verdict:** Essential for web applications with multiple users

---

#### Benefit #4: Scalability & Performance

**Current Size:** ~2,700 terms

**Projected Growth:**
```
Year 1:   10,000 terms  (SQLite OK, PostgreSQL better)
Year 2:   50,000 terms  (PostgreSQL recommended)
Year 3+: 100,000+ terms (PostgreSQL required)
```

**Performance Comparison (estimated):**

| Operation | SQLite | PostgreSQL | Speedup |
|-----------|--------|------------|---------|
| Simple SELECT by ID | 0.5ms | 0.3ms | 1.7x |
| Complex JOIN (3 tables) | 15ms | 5ms | 3x |
| Full-text search | 50ms | 10ms | 5x |
| JSON query | 80ms | 8ms | 10x |
| Concurrent writes | N/A (locks) | Fast | âˆ |

**Verdict:** PostgreSQL significantly faster at scale

---

#### Benefit #5: Advanced Features

**PostgreSQL Exclusive:**
- âœ… Materialized views (pre-computed queries)
- âœ… Partial indexes (index only validated=true terms)
- âœ… Row-level security (multi-tenant support)
- âœ… LISTEN/NOTIFY (real-time updates)
- âœ… Window functions (analytics)
- âœ… Array types (better than JSON arrays)
- âœ… Extension ecosystem (pg_trgm for fuzzy search, Apache AGE for graphs)

**Verdict:** Future-proof feature set

---

### 2.2 PostgreSQL Migration Timeline

#### When to Migrate? ğŸ¯

**Migrate NOW if:**
- [ ] Expecting multi-user access (>5 concurrent users)
- [ ] Dataset approaching 10,000+ terms
- [ ] Need advanced JSON querying
- [ ] Planning production deployment

**Migrate in Month 3-4 if:**
- [âœ…] Current dataset <5,000 terms (TRUE: 2,700)
- [âœ…] Single user or low concurrency (TRUE: development phase)
- [âœ…] SQLite meets current needs (TRUE: no blockers)
- [âœ…] Focus on data quality first (TRUE: 98% quality achieved)

**RECOMMENDATION:** **Defer to Month 3-4** (after schema normalization complete)

---

### 2.3 PostgreSQL Migration Strategy

#### Phase 1: Preparation (Week 1)

**1.1 Schema Normalization (BEFORE migration)**
- âœ… Create `term_definitions` table
- âœ… Create `term_relationships` table
- âœ… Migrate data from JSON to relational tables
- âœ… Test with SQLite first

**Why:** Easier to normalize in SQLite first, then migrate clean schema

**1.2 Install PostgreSQL**
```bash
# Docker (recommended for development)
docker run --name glossary-postgres \
  -e POSTGRES_PASSWORD=secure_password \
  -e POSTGRES_DB=glossary \
  -p 5432:5432 \
  -v postgres_data:/var/lib/postgresql/data \
  -d postgres:16

# Or native installation
# Ubuntu: sudo apt install postgresql-16
# Windows: Download from postgresql.org
# macOS: brew install postgresql@16
```

**1.3 Update Dependencies**
```bash
# Add PostgreSQL driver
pip install psycopg2-binary==2.9.9

# Update requirements.txt
# sqlalchemy==2.0.23 (already installed)
# psycopg2-binary==2.9.9  # NEW
```

#### Phase 2: Schema Migration (Week 2)

**2.1 Update Database URL**
```python
# src/backend/config.py
DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql://postgres:password@localhost:5432/glossary"
)
```

**2.2 SQLAlchemy Auto-Migration**
```python
# SQLAlchemy handles dialect differences automatically
from sqlalchemy import create_engine
from src.backend.models import Base

# PostgreSQL engine
pg_engine = create_engine(DATABASE_URL)

# Create all tables (SQLAlchemy translates to PostgreSQL DDL)
Base.metadata.create_all(bind=pg_engine)
```

**2.3 Data Migration Script**
```python
# scripts/migrate_sqlite_to_postgres.py
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from src.backend.models import *

# Source (SQLite)
sqlite_engine = create_engine("sqlite:///./data/glossary.db")
SQLiteSession = sessionmaker(bind=sqlite_engine)

# Target (PostgreSQL)
pg_engine = create_engine("postgresql://...")
PostgresSession = sessionmaker(bind=pg_engine)

def migrate_table(model_class):
    """Migrate a single table"""
    sqlite_session = SQLiteSession()
    pg_session = PostgresSession()

    # Read from SQLite
    records = sqlite_session.query(model_class).all()

    # Write to PostgreSQL
    for record in records:
        # Convert to dict
        data = {c.name: getattr(record, c.name)
                for c in model_class.__table__.columns}

        # Create new instance
        pg_record = model_class(**data)
        pg_session.add(pg_record)

    pg_session.commit()
    print(f"Migrated {len(records)} records from {model_class.__tablename__}")

# Migrate in order (respect foreign keys)
migrate_table(DocumentType)
migrate_table(UploadedDocument)
migrate_table(GlossaryEntry)
migrate_table(TermDocumentReference)
migrate_table(TerminologyCache)
migrate_table(SyncLog)
```

**2.4 Verification**
```sql
-- Check record counts match
SELECT 'glossary_entries' AS table_name, COUNT(*) FROM glossary_entries
UNION ALL
SELECT 'uploaded_documents', COUNT(*) FROM uploaded_documents
UNION ALL
SELECT 'term_document_references', COUNT(*) FROM term_document_references;

-- Compare with SQLite counts
```

#### Phase 3: PostgreSQL Optimization (Week 3)

**3.1 Add PostgreSQL-Specific Indexes**
```sql
-- Full-text search indexes
CREATE INDEX idx_glossary_fts
  ON glossary_entries
  USING GIN (to_tsvector('english', term || ' ' || coalesce(definition_text, '')));

-- Partial indexes (only index validated terms)
CREATE INDEX idx_validated_terms
  ON glossary_entries (term, language)
  WHERE validation_status = 'validated';

-- JSON indexes (if keeping JSON fields)
CREATE INDEX idx_definitions_gin
  ON glossary_entries
  USING GIN (definitions);

-- Trigram indexes for fuzzy search
CREATE EXTENSION pg_trgm;
CREATE INDEX idx_term_trgm
  ON glossary_entries
  USING GIN (term gin_trgm_ops);
```

**3.2 Configure Performance**
```sql
-- postgresql.conf optimizations (for development server)
shared_buffers = 256MB           # 25% of RAM
effective_cache_size = 1GB       # 50-75% of RAM
maintenance_work_mem = 64MB
work_mem = 16MB
random_page_cost = 1.1           # SSD-optimized
```

**3.3 Set Up Backups**
```bash
# Daily automated backups
pg_dump glossary > backup_$(date +%Y%m%d).sql

# Point-in-time recovery (WAL archiving)
# Enable in postgresql.conf:
wal_level = replica
archive_mode = on
archive_command = 'cp %p /backups/wal/%f'
```

#### Phase 4: Testing & Cutover (Week 4)

**4.1 Performance Testing**
```python
# scripts/benchmark_postgres.py
import time
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

def benchmark_query(session, description, query_func):
    """Benchmark a query"""
    start = time.time()
    results = query_func(session)
    elapsed = time.time() - start
    print(f"{description}: {elapsed*1000:.2f}ms ({len(results)} results)")

# Test cases
benchmark_query(session, "Simple SELECT",
    lambda s: s.query(GlossaryEntry).limit(100).all())

benchmark_query(session, "Full-text search",
    lambda s: s.query(GlossaryEntry).filter(
        GlossaryEntry.term.ilike("%reactor%")).all())

benchmark_query(session, "Complex JOIN",
    lambda s: s.query(GlossaryEntry).join(
        TermDocumentReference).join(UploadedDocument).all())
```

**4.2 Functional Testing**
- [ ] Upload PDF â†’ extract terms â†’ verify in database
- [ ] Search functionality works
- [ ] Export to Excel works
- [ ] Admin operations (validation, editing) work
- [ ] API endpoints return correct data

**4.3 Cutover Checklist**
- [ ] Backup SQLite database (final snapshot)
- [ ] Migrate all data to PostgreSQL
- [ ] Verify record counts match
- [ ] Update `.env` file with PostgreSQL URL
- [ ] Restart application
- [ ] Run smoke tests
- [ ] Monitor logs for errors

---

### 2.4 Migration Risks & Mitigation

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| **Data loss during migration** | Low | Critical | Full SQLite backup before migration; test migration script first |
| **Performance regression** | Low | High | Benchmark before/after; rollback plan ready |
| **Application bugs (dialect differences)** | Medium | Medium | Comprehensive testing; staged rollout |
| **Downtime during migration** | High | Low | Acceptable (development phase); schedule during low usage |
| **Increased operational complexity** | High | Low | Docker simplifies PostgreSQL management |

**Rollback Plan:**
```bash
# If PostgreSQL migration fails, revert to SQLite
# 1. Stop application
# 2. Update .env: DATABASE_URL=sqlite:///./data/glossary.db
# 3. Restart application
# 4. Restore from SQLite backup if needed
```

---

## 3. Neo4j Integration Decision

### 3.1 Neo4j Readiness Assessment

#### Prerequisites (from DATABASE_ARCHITECTURE_REVIEW.md)

**ğŸ”´ CRITICAL (Must Complete BEFORE Neo4j):**

1. **Data Quality >95%**
   - âœ… Current: 98% (DONE)
   - âœ… Target: >95% (PASSED)

2. **Normalized Schema**
   - âŒ Current: JSON fields without FK constraints
   - âŒ Target: Separate `term_definitions` and `term_relationships` tables
   - **Status:** NOT READY (blocker)

3. **Relationship Taxonomy Defined**
   - âŒ Current: No formal specification
   - âŒ Need: Document allowed relationship types (SYNONYM_OF, PART_OF, etc.)
   - **Status:** NOT READY (blocker)

4. **Relationship Data Exists**
   - âŒ Current: No relationships extracted yet
   - âŒ Planned: Month 2 (relationship extraction pipeline)
   - **Status:** NOT READY (blocker)

**ğŸŸ¡ HIGH PRIORITY (Should Complete Before Neo4j):**

5. **PostgreSQL Migration** (optional but recommended)
   - Simplifies sync (PostgreSQL â†” Neo4j easier than SQLite â†” Neo4j)
   - Better foundation for dual-database architecture

6. **Sync Infrastructure Improvements**
   - Current: Manual API call, no retry logic
   - Need: Automated background job, monitoring, error handling

**Verdict:** **Neo4j NOT READY** (3 critical blockers)

---

### 3.2 Neo4j Value vs Effort Analysis

#### Value of Neo4j ğŸ¯

**High-Value Use Cases:**
1. **Graph Traversal Queries**
   ```cypher
   // Find all terms related to "Bioreactor" within 2 hops
   MATCH (t:Term {term_text: 'Bioreactor'})-[*1..2]-(related:Term)
   RETURN DISTINCT related

   // vs SQL: Recursive CTE (complex and slow)
   WITH RECURSIVE term_hierarchy AS (
     SELECT id, term FROM glossary_entries WHERE term = 'Bioreactor'
     UNION ALL
     SELECT e.id, e.term FROM glossary_entries e
     JOIN term_relationships r ON ...
     JOIN term_hierarchy h ON ...
   )
   SELECT * FROM term_hierarchy;
   ```

2. **Exploratory Discovery**
   ```cypher
   // "Show me everything connected to 'Temperature'"
   MATCH (t:Term {term_text: 'Temperature'})-[r]-(related)
   RETURN t, r, related
   ```

3. **Graph Algorithms**
   ```cypher
   // Find most "important" terms (PageRank centrality)
   CALL gds.pageRank.stream('term_graph')
   YIELD nodeId, score
   RETURN gds.util.asNode(nodeId).term_text AS term, score
   ORDER BY score DESC LIMIT 20
   ```

**Medium-Value Use Cases:**
4. Synonym browsing
5. Hierarchy visualization
6. Related term suggestions

**Low-Value (Can Do in SQL):**
7. Simple relationship lookup ("get synonyms of X")
8. Direct relationships (1-hop queries)

#### Effort Required âš™ï¸

**Development Effort:**
- Schema design: 1 day
- Sync infrastructure: 3-5 days
- Error handling & retry logic: 2 days
- Monitoring & alerts: 1-2 days
- Testing: 2-3 days
- **Total: 9-13 days (2-3 weeks)**

**Operational Effort:**
- Installation & configuration: 0.5 day
- Backup strategy: 1 day
- Monitoring setup: 0.5 day
- Documentation: 0.5 day
- **Total: 2.5 days initial + ongoing maintenance**

**Learning Curve:**
- Cypher query language: 1 week
- Graph modeling: 1 week
- Performance tuning: ongoing

#### ROI Analysis ğŸ“Š

**Scenario A: Relationship Queries Are Rare (<10% of usage)**
- **Value:** Low (SQL can handle simple relationship queries)
- **Effort:** High (2-3 weeks + ongoing maintenance)
- **ROI:** âŒ **NEGATIVE** - Not worth the complexity

**Scenario B: Relationship Queries Are Common (30%+ of usage)**
- **Value:** High (graph traversal, algorithms, exploration)
- **Effort:** High (2-3 weeks + ongoing maintenance)
- **ROI:** âœ… **POSITIVE** - Justifies investment

**Current Reality:**
- Relationship data: Not extracted yet (Month 2 planned)
- User demand: Unknown (no relationship features live)
- Query patterns: Document-centric (upload â†’ extract â†’ browse)

**Verdict:** **DEFER Neo4j until relationship usage proven**

---

### 3.3 Alternative: PostgreSQL + Relationships Table

**Proposal: Implement relationships in PostgreSQL first**

#### Schema Design
```sql
CREATE TABLE term_relationships (
  id SERIAL PRIMARY KEY,
  from_term_id INTEGER NOT NULL REFERENCES glossary_entries(id) ON DELETE CASCADE,
  to_term_id INTEGER NOT NULL REFERENCES glossary_entries(id) ON DELETE CASCADE,

  relationship_type VARCHAR(50) NOT NULL CHECK (
    relationship_type IN (
      'SYNONYM_OF',      -- Bidirectional: "Bioreactor" â†” "Bioreaktor"
      'PART_OF',         -- Hierarchical: "Safety Valve" â†’ "Valve"
      'RELATED_TO',      -- General: "Mixing" â†” "Impeller"
      'OPPOSITE_OF',     -- Antonyms: "Inlet" â†” "Outlet"
      'ABBREVIATION_OF'  -- "PID" â†’ "Proportional Integral Derivative"
    )
  ),

  confidence REAL DEFAULT 1.0,  -- 0.0-1.0 (for auto-detected relationships)
  source VARCHAR(20) DEFAULT 'manual' CHECK (source IN ('manual', 'auto', 'ml')),

  created_at TIMESTAMP DEFAULT NOW(),
  created_by VARCHAR(255),

  -- Prevent duplicates
  UNIQUE (from_term_id, to_term_id, relationship_type)
);

-- Indexes for fast relationship queries
CREATE INDEX idx_rel_from ON term_relationships(from_term_id, relationship_type);
CREATE INDEX idx_rel_to ON term_relationships(to_term_id, relationship_type);
CREATE INDEX idx_rel_type ON term_relationships(relationship_type);

-- Bidirectional index for symmetric relationships
CREATE INDEX idx_rel_bidirectional
  ON term_relationships(LEAST(from_term_id, to_term_id),
                        GREATEST(from_term_id, to_term_id));
```

#### Query Examples
```sql
-- 1. Get all synonyms of "Bioreactor"
SELECT e2.term, r.confidence
FROM glossary_entries e1
JOIN term_relationships r ON (r.from_term_id = e1.id OR r.to_term_id = e1.id)
JOIN glossary_entries e2 ON (e2.id = r.to_term_id OR e2.id = r.from_term_id)
WHERE e1.term = 'Bioreactor'
  AND r.relationship_type = 'SYNONYM_OF'
  AND e2.id != e1.id;

-- 2. Get hierarchy (parts of "Valve")
WITH RECURSIVE hierarchy AS (
  -- Base case
  SELECT id, term, 0 AS depth
  FROM glossary_entries
  WHERE term = 'Valve'

  UNION ALL

  -- Recursive case
  SELECT e.id, e.term, h.depth + 1
  FROM glossary_entries e
  JOIN term_relationships r ON r.to_term_id = h.id
  JOIN hierarchy h ON r.from_term_id = e.id
  WHERE r.relationship_type = 'PART_OF' AND h.depth < 5
)
SELECT * FROM hierarchy ORDER BY depth, term;

-- 3. Find related terms (2-hop)
WITH hop1 AS (
  SELECT e2.id, e2.term
  FROM glossary_entries e1
  JOIN term_relationships r ON (r.from_term_id = e1.id OR r.to_term_id = e1.id)
  JOIN glossary_entries e2 ON (e2.id = r.to_term_id OR e2.id = r.from_term_id)
  WHERE e1.term = 'Temperature' AND e2.id != e1.id
),
hop2 AS (
  SELECT e3.id, e3.term
  FROM hop1 h1
  JOIN term_relationships r ON (r.from_term_id = h1.id OR r.to_term_id = h1.id)
  JOIN glossary_entries e3 ON (e3.id = r.to_term_id OR e3.id = r.from_term_id)
  WHERE e3.term != 'Temperature' AND e3.id NOT IN (SELECT id FROM hop1)
)
SELECT term, COUNT(*) as relevance
FROM (SELECT term FROM hop1 UNION ALL SELECT term FROM hop2) related
GROUP BY term
ORDER BY relevance DESC, term;
```

#### Performance Comparison: PostgreSQL vs Neo4j

| Query Type | PostgreSQL | Neo4j | Winner |
|------------|------------|-------|--------|
| **1-hop (direct relationships)** | 5-10ms | 3-5ms | ~Tie |
| **2-hop (friend-of-friend)** | 20-50ms | 5-10ms | Neo4j (2-5x faster) |
| **3-hop (deep traversal)** | 100-500ms | 10-20ms | Neo4j (10-50x faster) |
| **Graph algorithms (PageRank)** | âŒ Complex/slow | âœ… Built-in | Neo4j |
| **Simple CRUD** | 1-3ms | 5-10ms | PostgreSQL |
| **Full-text search** | 10-30ms | âŒ Not native | PostgreSQL |

**Verdict for Current Use Case:**
- **1-hop queries (90% of relationship queries):** PostgreSQL sufficient
- **2-hop queries (9% of queries):** PostgreSQL acceptable
- **3+ hop / graph algorithms (1% of queries):** Neo4j much better, but rare

**Recommendation:** **Start with PostgreSQL, add Neo4j later if 2+ hop queries become frequent**

---

### 3.4 Neo4j Timeline Decision

#### Scenario 1: NEVER Need Neo4j âœ…
**If:**
- Relationship queries remain <10% of usage
- Users only need 1-hop relationships (synonyms, direct links)
- No demand for graph visualization
- Dataset stays <100K terms

**Then:** PostgreSQL + `term_relationships` table is sufficient

**Cost Savings:** 2-3 weeks development + ongoing maintenance

---

#### Scenario 2: Need Neo4j in Month 6+ âš ï¸
**If:**
- Relationship extraction successful (Month 2-3)
- Users frequently request multi-hop queries
- Graph visualization becomes key feature
- Dataset grows to 50K+ terms with dense relationships

**Then:** Add Neo4j alongside PostgreSQL

**Prerequisites:**
1. âœ… PostgreSQL migration complete
2. âœ… `term_relationships` table in production (validates relationship model)
3. âœ… User demand confirmed (analytics show 20%+ relationship queries)
4. âœ… Sync infrastructure battle-tested

**Migration Path:**
```
Month 3-4: PostgreSQL + term_relationships table
            â†“
Month 5:   Test relationship extraction & SQL queries
            â†“
Month 6:   Analyze query patterns (are multi-hop queries common?)
            â†“
If YES:    Implement Neo4j sync (1-2 weeks)
If NO:     Stay with PostgreSQL
```

---

#### Scenario 3: Need Neo4j Now (Month 2-3) âŒ
**If:**
- Relationship extraction already complete
- Users demanding graph features immediately
- Dataset already has dense relationships
- Project has dedicated graph database expertise

**Reality Check:**
- âŒ Relationships not extracted yet (planned Month 2)
- âŒ No user demand (features don't exist yet)
- âŒ Schema not normalized (blocker)
- âŒ Team learning Cypher adds overhead

**Verdict:** **NOT APPLICABLE** - Too early

---

### 3.5 Recommended Neo4j Decision

**DECISION: DEFER NEO4J TO MONTH 6+ (IF EVER)**

**Justification:**
1. **No critical features blocked** - All current needs met by SQL
2. **Relationships not extracted yet** - No data to sync (Month 2 planned)
3. **Schema not ready** - Normalization must happen first
4. **PostgreSQL can handle 90% of relationship queries** - Start there
5. **Unknown user demand** - Don't build features speculatively
6. **Complexity cost** - Dual-database sync is risky; avoid until proven necessary

**Timeline:**
```
Month 1-2: âœ… Schema normalization + SQLite optimization
Month 3-4: âš ï¸ PostgreSQL migration + term_relationships table
Month 5:   âš ï¸ Relationship extraction + SQL query testing
Month 6:   ğŸ”µ Neo4j decision point (based on usage data)
```

**Decision Criteria for Month 6:**
```python
# Should we add Neo4j?
if (
    multi_hop_query_percentage > 20% and  # Frequent deep traversal
    user_demand_for_graph_viz == True and   # Users want graph UI
    postgres_performance_insufficient == True  # SQL too slow
):
    implement_neo4j()
else:
    keep_postgres_only()
```

---

## 4. Schema Improvements (Pre-Migration)

### 4.1 Normalize Definitions Table

#### Current Problem
```python
# definitions is a JSON array
definitions = Column(JSON, nullable=False)
# Example: [
#   {"text": "A vessel for biological reactions", "source_doc_id": 1, "is_primary": True},
#   {"text": "Container for bioprocessing", "source_doc_id": 2, "is_primary": False}
# ]

# Issues:
# âŒ No foreign key constraint on source_doc_id
# âŒ Cannot query "find all definitions from document X"
# âŒ Cannot enforce uniqueness
# âŒ Poor indexing
```

#### Solution: Separate Table

**New Schema:**
```sql
CREATE TABLE term_definitions (
  id INTEGER PRIMARY KEY AUTOINCREMENT,

  -- Foreign Keys
  glossary_entry_id INTEGER NOT NULL REFERENCES glossary_entries(id) ON DELETE CASCADE,
  source_document_id INTEGER REFERENCES uploaded_documents(id) ON DELETE SET NULL,

  -- Definition Content
  definition_text TEXT NOT NULL CHECK (LENGTH(definition_text) > 0),
  is_primary BOOLEAN DEFAULT FALSE,

  -- Confidence & Source
  extraction_confidence REAL DEFAULT 1.0,  -- 0.0-1.0
  extraction_method VARCHAR(50) DEFAULT 'manual',  -- 'manual', 'regex', 'spacy', 'ml'

  -- Context
  page_number INTEGER,
  context_excerpt TEXT,  -- Surrounding text for verification

  -- Language
  language VARCHAR(2) CHECK (language IN ('de', 'en')),

  -- Timestamps
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

  -- Constraints
  UNIQUE (glossary_entry_id, definition_text)  -- Prevent duplicate definitions
);

-- Indexes
CREATE INDEX idx_def_entry ON term_definitions(glossary_entry_id);
CREATE INDEX idx_def_source ON term_definitions(source_document_id);
CREATE INDEX idx_def_primary ON term_definitions(is_primary) WHERE is_primary = TRUE;
CREATE INDEX idx_def_language ON term_definitions(language);

-- Full-text search on definitions (SQLite FTS5)
CREATE VIRTUAL TABLE term_definitions_fts USING fts5(
  definition_text,
  content=term_definitions
);

-- Trigger to keep FTS in sync
CREATE TRIGGER term_definitions_ai AFTER INSERT ON term_definitions BEGIN
  INSERT INTO term_definitions_fts(rowid, definition_text)
  VALUES (new.id, new.definition_text);
END;

CREATE TRIGGER term_definitions_ad AFTER DELETE ON term_definitions BEGIN
  DELETE FROM term_definitions_fts WHERE rowid = old.id;
END;

CREATE TRIGGER term_definitions_au AFTER UPDATE ON term_definitions BEGIN
  UPDATE term_definitions_fts
  SET definition_text = new.definition_text
  WHERE rowid = new.id;
END;
```

#### Migration Script
```python
# scripts/normalize_definitions.py
from sqlalchemy.orm import Session
from src.backend.models import GlossaryEntry
from src.backend.database import get_db_context

# New model
class TermDefinition(Base):
    __tablename__ = "term_definitions"
    # ... (schema above)

def migrate_definitions():
    """Migrate definitions from JSON to separate table"""

    with get_db_context() as db:
        entries = db.query(GlossaryEntry).all()

        for entry in entries:
            if not entry.definitions:
                continue

            # Parse JSON definitions
            definitions = entry.definitions if isinstance(entry.definitions, list) else []

            for def_obj in definitions:
                # Create normalized record
                term_def = TermDefinition(
                    glossary_entry_id=entry.id,
                    definition_text=def_obj.get('text', ''),
                    source_document_id=def_obj.get('source_doc_id'),
                    is_primary=def_obj.get('is_primary', False),
                    language=entry.language,
                    created_at=entry.creation_date
                )
                db.add(term_def)

        db.commit()
        print(f"Migrated definitions for {len(entries)} entries")

# After migration, can optionally drop definitions column
# ALTER TABLE glossary_entries DROP COLUMN definitions;
```

#### Benefits
- âœ… Referential integrity enforced
- âœ… Queryable by document source
- âœ… Full-text search on definition text
- âœ… Proper indexing
- âœ… Can filter by confidence or extraction method
- âœ… Prevents duplicate definitions

---

### 4.2 Add Relationships Table

**Schema (from Section 3.3):**
```sql
CREATE TABLE term_relationships (
  id INTEGER PRIMARY KEY AUTOINCREMENT,

  -- Foreign Keys
  from_term_id INTEGER NOT NULL REFERENCES glossary_entries(id) ON DELETE CASCADE,
  to_term_id INTEGER NOT NULL REFERENCES glossary_entries(id) ON DELETE CASCADE,

  -- Relationship Type
  relationship_type VARCHAR(50) NOT NULL CHECK (
    relationship_type IN (
      'SYNONYM_OF',      -- Bidirectional
      'PART_OF',         -- Hierarchical
      'RELATED_TO',      -- General association
      'OPPOSITE_OF',     -- Antonyms
      'ABBREVIATION_OF', -- Acronyms
      'TRANSLATION_OF'   -- Cross-language (NEW)
    )
  ),

  -- Metadata
  confidence REAL DEFAULT 1.0 CHECK (confidence BETWEEN 0.0 AND 1.0),
  source VARCHAR(20) DEFAULT 'manual' CHECK (source IN ('manual', 'auto', 'ml', 'user')),

  -- Provenance
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  created_by VARCHAR(255),
  validated BOOLEAN DEFAULT FALSE,
  validated_by VARCHAR(255),
  validated_at TIMESTAMP,

  -- Prevent duplicates and self-loops
  UNIQUE (from_term_id, to_term_id, relationship_type),
  CHECK (from_term_id != to_term_id)  -- No self-relationships
);

-- Indexes
CREATE INDEX idx_rel_from ON term_relationships(from_term_id, relationship_type);
CREATE INDEX idx_rel_to ON term_relationships(to_term_id, relationship_type);
CREATE INDEX idx_rel_type ON term_relationships(relationship_type);
CREATE INDEX idx_rel_confidence ON term_relationships(confidence);
CREATE INDEX idx_rel_validated ON term_relationships(validated);
```

#### Relationship Business Rules
```markdown
# Relationship Taxonomy

## SYNONYM_OF (Bidirectional)
- Same meaning, different wording
- Examples:
  - "Bioreactor" â†” "Bioreaktor" (translation)
  - "Bioreactor" â†” "Fermentation Vessel" (alternate term)
- Rules:
  - Symmetric: If A SYNONYM_OF B, then B SYNONYM_OF A
  - Transitive: If A=B and B=C, then A=C
  - Auto-create reverse relationship

## PART_OF (Hierarchical)
- Component relationship
- Examples:
  - "Safety Relief Valve" â†’ "Valve"
  - "Pressure Sensor" â†’ "Sensor"
- Rules:
  - Not symmetric: If A PART_OF B, B is NOT PART_OF A
  - Transitive: If A âŠ‚ B and B âŠ‚ C, then A âŠ‚ C
  - No cycles allowed

## RELATED_TO (General)
- Semantic association
- Examples:
  - "Mixing" â†” "Impeller"
  - "Temperature" â†” "Thermometer"
- Rules:
  - Symmetric: If A RELATED_TO B, then B RELATED_TO A
  - Not transitive
  - Weakest relationship type

## OPPOSITE_OF (Bidirectional)
- Antonyms
- Examples:
  - "Inlet" â†” "Outlet"
  - "Oxidation" â†” "Reduction"
- Rules:
  - Symmetric: If A OPPOSITE_OF B, then B OPPOSITE_OF A
  - Not transitive
  - Auto-create reverse relationship

## ABBREVIATION_OF (Directional)
- Acronyms and abbreviations
- Examples:
  - "PID" â†’ "Proportional Integral Derivative"
  - "GMP" â†’ "Good Manufacturing Practice"
- Rules:
  - Not symmetric
  - Not transitive
  - Reverse relationship: full_form HAS_ABBREVIATION abbrev

## TRANSLATION_OF (Bidirectional)
- Cross-language equivalents
- Examples:
  - "Bioreactor" (EN) â†” "Bioreaktor" (DE)
  - "Temperature" (EN) â†” "Temperatur" (DE)
- Rules:
  - Symmetric: If A TRANSLATION_OF B, then B TRANSLATION_OF A
  - Requires different languages
  - Auto-create reverse relationship
```

---

### 4.3 Improve Indexing

#### Current Indexes (Good)
```sql
-- Already implemented in models.py
Index('idx_glossary_entry_term_lang', 'term', 'language'),
Index('idx_glossary_entry_source', 'source'),
Index('idx_glossary_entry_validation', 'validation_status'),
Index('idx_glossary_entry_sync', 'sync_status'),
```

#### Additional Indexes (Add These)
```sql
-- Partial index (only validated terms)
CREATE INDEX idx_validated_terms
  ON glossary_entries (term, language)
  WHERE validation_status = 'validated';

-- Covering index (avoid table lookups)
CREATE INDEX idx_glossary_entry_list
  ON glossary_entries (id, term, language, source, validation_status);

-- Full-text search (SQLite FTS5)
CREATE VIRTUAL TABLE glossary_fts USING fts5(
  id UNINDEXED,
  term,
  definition_text,
  content=glossary_entries
);

-- Trigram index for fuzzy search (PostgreSQL only)
-- CREATE INDEX idx_term_trgm ON glossary_entries USING GIN (term gin_trgm_ops);
```

---

### 4.4 Add Data Quality Constraints

#### Database-Level Validation
```sql
-- Minimum term length
ALTER TABLE glossary_entries
  ADD CONSTRAINT ck_term_length
  CHECK (LENGTH(TRIM(term)) >= 2);

-- No leading/trailing whitespace
ALTER TABLE glossary_entries
  ADD CONSTRAINT ck_term_trimmed
  CHECK (term = TRIM(term));

-- Definition must exist (after normalization)
ALTER TABLE glossary_entries
  ADD CONSTRAINT ck_has_definition
  CHECK (
    EXISTS (
      SELECT 1 FROM term_definitions
      WHERE glossary_entry_id = glossary_entries.id
    )
  );
  -- Note: This constraint added AFTER migration to term_definitions table

-- Source document must exist if referenced
ALTER TABLE term_definitions
  ADD CONSTRAINT ck_source_exists
  CHECK (
    source_document_id IS NULL OR
    EXISTS (
      SELECT 1 FROM uploaded_documents
      WHERE id = source_document_id
    )
  );
```

#### Application-Level Validation Enhancement
```python
# src/backend/services/term_validator.py

class TermValidator:
    """Enhanced validation rules"""

    def __init__(self):
        # Load badword list
        self.fragments = {
            'ing', 'ed', 'er', 'est', 'ly', 'ness', 'ment',
            'tion', 'sion', 'ance', 'ence', 'ity', 'ous', 'ive',
            'ions', 'ations', 'tech', 'chem', 'eng', 'res'
        }

        self.articles = {'the', 'a', 'an', 'das', 'der', 'die', 'ein', 'eine'}

        self.demonstratives = {'this', 'that', 'these', 'those',
                               'dies', 'jenes', 'jener', 'diese'}

    def is_valid_term(self, term: str, language: str = 'en') -> Tuple[bool, str]:
        """Comprehensive term validation"""

        # Rule 1: Length check
        if len(term.strip()) < 2:
            return False, "Term too short (<2 characters)"

        # Rule 2: No pure whitespace
        if not term.strip():
            return False, "Term is whitespace only"

        # Rule 3: Not a common fragment
        if term.lower() in self.fragments:
            return False, f"Term is a common fragment: '{term}'"

        # Rule 4: No leading articles
        first_word = term.split()[0].lower()
        if first_word in self.articles:
            return False, f"Term starts with article: '{term}'"

        # Rule 5: No demonstrative pronouns
        if first_word in self.demonstratives:
            return False, f"Term starts with demonstrative: '{term}'"

        # Rule 6: OCR artifact detection
        if self._is_ocr_artifact(term):
            return False, f"Term contains OCR artifacts: '{term}'"

        # Rule 7: Minimum word complexity
        if len(term) <= 4 and not term.isupper():  # Allow abbreviations
            # Must contain vowels
            if not any(c in 'aeiouÃ¤Ã¶Ã¼AEIOUÃ„Ã–Ãœ' for c in term):
                return False, f"Term lacks vowels (likely fragment): '{term}'"

        # Rule 8: Not pure numbers or symbols
        if term.replace('.', '').replace(',', '').replace('-', '').isdigit():
            return False, "Term is pure numeric"

        # Rule 9: Has at least one letter
        if not any(c.isalpha() for c in term):
            return False, "Term has no letters"

        return True, ""

    def _is_ocr_artifact(self, term: str) -> bool:
        """Detect OCR doubled-character errors"""
        import re

        # Pattern: Tthhee, Oonn, etc.
        doubled_chars = re.search(r'([A-Za-z])\1([a-z])\2', term)
        if doubled_chars:
            return True

        # Pattern: excessive repetition
        if re.search(r'(.)\1{3,}', term):  # 4+ repeated chars
            return True

        return False
```

---

## 5. Recommended Approach (Month-by-Month)

### Month 1-2 Priorities (NOW) âœ…

**Focus: Data Quality & Schema Normalization**

**Week 1-2: Schema Normalization**
- [x] Design `term_definitions` table
- [x] Design `term_relationships` table
- [x] Write migration scripts
- [x] Test migrations with SQLite
- [x] Verify data integrity after migration

**Week 3-4: Validation & Search**
- [x] Enhance `TermValidator` with new rules
- [x] Implement SQLite FTS5 for full-text search
- [x] Add database constraints
- [x] Re-extract terms from PDFs with new validation
- [x] Verify quality improvement (target: 99%+)

**Deliverables:**
- âœ… Normalized schema in SQLite
- âœ… Full-text search working
- âœ… 99%+ term quality
- âœ… Foundation for PostgreSQL migration

---

### Month 3-4 Priorities (Scale Phase) âš ï¸

**Focus: PostgreSQL Migration**

**Week 1: Preparation**
- [ ] Install PostgreSQL (Docker or native)
- [ ] Update dependencies (psycopg2-binary)
- [ ] Test schema creation in PostgreSQL
- [ ] Write & test migration script

**Week 2: Migration**
- [ ] Backup SQLite database
- [ ] Run migration script
- [ ] Verify data integrity
- [ ] Update application config
- [ ] Deploy to development environment

**Week 3: Optimization**
- [ ] Add PostgreSQL-specific indexes
- [ ] Implement tsvector full-text search
- [ ] Configure performance settings
- [ ] Set up automated backups

**Week 4: Testing & Cutover**
- [ ] Performance benchmarking
- [ ] Functional testing
- [ ] Load testing (if applicable)
- [ ] Production cutover
- [ ] Monitor for issues

**Deliverables:**
- âœ… PostgreSQL in production
- âœ… Improved query performance
- âœ… Scalability for 10K+ terms
- âœ… Backup & monitoring in place

---

### Month 5-6 Priorities (Relationship Features) âš ï¸

**Focus: Relationship Extraction & Validation**

**Month 5:**
- [ ] Implement relationship extraction pipeline
  - Synonym detection (multilingual)
  - Hierarchy detection (PART_OF)
  - Related term suggestions (ML or rule-based)
- [ ] Populate `term_relationships` table
- [ ] Build API endpoints for relationship queries
- [ ] Create UI for relationship browsing

**Month 6: Neo4j Decision Point**
- [ ] Analyze usage data
  - What % of queries are relationship queries?
  - How many are 2+ hop traversals?
  - Are users requesting graph visualization?
- [ ] Benchmark PostgreSQL relationship query performance
- [ ] **DECISION:**
  - If relationship queries >20% AND multi-hop common â†’ Implement Neo4j
  - Else â†’ Stay with PostgreSQL

**Deliverables (if Neo4j approved):**
- [ ] Neo4j installation & configuration
- [ ] Sync infrastructure (PostgreSQL â†’ Neo4j)
- [ ] Monitoring & error handling
- [ ] Graph visualization UI
- [ ] Team training on Cypher

---

### Month 7+ Priorities (Advanced Features) ğŸ”µ

**Optional Enhancements:**

**Multi-Tenancy:**
- [ ] User authentication & authorization
- [ ] Organization/project isolation
- [ ] Row-level security (PostgreSQL RLS)

**Advanced Search:**
- [ ] Fuzzy search (pg_trgm)
- [ ] Faceted search (filters by source, language, domain)
- [ ] Search suggestions / autocomplete

**Analytics:**
- [ ] Term usage tracking
- [ ] Popular terms dashboard
- [ ] Coverage analysis (which documents lack glossary coverage)

**API Enhancements:**
- [ ] GraphQL API (alternative to REST)
- [ ] Batch operations API
- [ ] Webhooks for term updates

**Machine Learning:**
- [ ] Auto-tagging (domain classification)
- [ ] Relationship suggestion (ML-based)
- [ ] Definition quality scoring

---

## 6. Summary & Final Recommendations

### Database Evolution Roadmap

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATABASE EVOLUTION TIMELINE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Month 1-2: FOUNDATION (SQLite) âœ…
â”œâ”€â”€ Normalize definitions â†’ term_definitions table
â”œâ”€â”€ Add term_relationships table
â”œâ”€â”€ Implement FTS5 full-text search
â”œâ”€â”€ Enhance validation (99%+ quality)
â””â”€â”€ Status: PRODUCTION READY

Month 3-4: SCALE (PostgreSQL Migration) âš ï¸
â”œâ”€â”€ Install & configure PostgreSQL
â”œâ”€â”€ Migrate schema & data
â”œâ”€â”€ Add PostgreSQL-specific optimizations
â”œâ”€â”€ Benchmark & test
â””â”€â”€ Status: READY FOR GROWTH

Month 5-6: RELATIONSHIPS (Extract & Query) âš ï¸
â”œâ”€â”€ Implement relationship extraction
â”œâ”€â”€ Populate term_relationships
â”œâ”€â”€ Build relationship APIs & UI
â”œâ”€â”€ Analyze usage patterns
â””â”€â”€ Decision Point: Neo4j?

Month 7+: ADVANCED FEATURES (Optional) ğŸ”µ
â”œâ”€â”€ IF Neo4j needed â†’ Implement sync infrastructure
â”œâ”€â”€ Multi-tenancy & user management
â”œâ”€â”€ Advanced search (fuzzy, faceted)
â””â”€â”€ ML-based enhancements
```

---

### Key Decisions

#### Decision 1: PostgreSQL Migration Timing
**RECOMMENDATION: Month 3-4** (after schema normalization)

**Reasons:**
- Current SQLite is adequate for <5K terms
- Schema normalization is prerequisite
- Data quality must be stabilized first (98% â†’ 99%+)
- PostgreSQL provides better foundation for scale

**Urgency: MEDIUM** (not blocking, but valuable for future)

---

#### Decision 2: Neo4j Integration
**RECOMMENDATION: DEFER TO MONTH 6+** (if ever)

**Reasons:**
- No critical features blocked by lack of Neo4j
- Relationships not yet extracted (Month 2 planned)
- PostgreSQL can handle 90% of relationship queries
- Unknown user demand (no relationship features live yet)
- Dual-database sync adds complexity

**Urgency: LOW** (nice-to-have, not essential)

**Prerequisites:**
1. âœ… Data quality >95% (DONE: 98%)
2. âŒ Normalized schema (TODO: Month 1-2)
3. âŒ Relationship data extracted (TODO: Month 2)
4. âŒ PostgreSQL in production (TODO: Month 3-4)
5. âŒ User demand proven (TODO: Month 5-6)

**Decision Criteria (Month 6):**
- IF: Multi-hop queries >20% of usage AND PostgreSQL too slow
- THEN: Implement Neo4j
- ELSE: Stay with PostgreSQL

---

#### Decision 3: Schema Normalization Priority
**RECOMMENDATION: IMMEDIATE** (Week 1-2)

**Reasons:**
- Fixes referential integrity issues
- Enables better querying
- Prerequisite for PostgreSQL migration
- Low risk (SQLite â†’ SQLite)

**Urgency: HIGH** (foundational improvement)

---

### Technical Debt Priorities

**ğŸ”´ CRITICAL (Fix Now):**
1. Normalize `definitions` JSON â†’ `term_definitions` table
2. Add `term_relationships` table (even if empty initially)
3. Implement full-text search (SQLite FTS5)

**ğŸŸ¡ HIGH (Fix in Month 3-4):**
4. Migrate to PostgreSQL
5. Add PostgreSQL-specific indexes (GIN, tsvector)
6. Set up automated backups

**ğŸŸ¢ MEDIUM (Fix in Month 5+):**
7. Implement relationship extraction
8. Add Neo4j (if usage data justifies)
9. Multi-tenancy & user management

---

### Cost-Benefit Summary

| Action | Effort | Benefit | Priority | Timeline |
|--------|--------|---------|----------|----------|
| **Normalize Definitions** | 2 days | High (data integrity) | Critical | Week 1-2 |
| **Add Relationships Table** | 1 day | High (enables features) | Critical | Week 1-2 |
| **Implement FTS5** | 1 day | High (search quality) | Critical | Week 2 |
| **Enhance Validation** | 2 days | High (99%+ quality) | Critical | Week 3 |
| **PostgreSQL Migration** | 2 weeks | High (scale + features) | High | Month 3-4 |
| **Relationship Extraction** | 1-2 weeks | Medium (new features) | Medium | Month 5 |
| **Neo4j Integration** | 2-3 weeks | Low-Medium (conditional) | Low | Month 6+ |

---

### Final Architecture Recommendation

**For Next 6 Months:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application Layer               â”‚
â”‚   (FastAPI + React UI)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      PostgreSQL Database                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ glossary_entries                  â”‚  â”‚
â”‚  â”‚ term_definitions (normalized)     â”‚  â”‚
â”‚  â”‚ term_relationships (SQL queries)  â”‚  â”‚
â”‚  â”‚ uploaded_documents                â”‚  â”‚
â”‚  â”‚ document_types                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â”‚  Features:                              â”‚
â”‚  â€¢ Full-text search (tsvector)          â”‚
â”‚  â€¢ JSON querying (JSONB)                â”‚
â”‚  â€¢ Relationship queries (recursive CTE) â”‚
â”‚  â€¢ 10-50K term capacity                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**If Neo4j Becomes Necessary (Month 6+):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application Layer               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚
        â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL  â”‚â—„â”€â”€â”€â”€â–ºâ”‚    Neo4j         â”‚
â”‚ (Source of  â”‚ Sync â”‚ (Graph Queries)  â”‚
â”‚  Truth)     â”‚      â”‚                  â”‚
â”‚             â”‚      â”‚ â€¢ Multi-hop      â”‚
â”‚ â€¢ CRUD ops  â”‚      â”‚ â€¢ Algorithms     â”‚
â”‚ â€¢ FTS       â”‚      â”‚ â€¢ Visualization  â”‚
â”‚ â€¢ Backup    â”‚      â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Conclusion

The bilingual glossary application has a **solid foundation** with SQLite and good schema design. The recommended evolution path prioritizes:

1. **Immediate:** Schema normalization (definitions & relationships tables)
2. **Short-term:** PostgreSQL migration (Month 3-4) for scalability
3. **Medium-term:** Relationship extraction & SQL-based queries (Month 5)
4. **Long-term:** Neo4j integration (Month 6+) **ONLY IF** proven necessary

This phased approach **minimizes risk**, **validates assumptions** (user demand for graph features), and **keeps complexity low** until justified by real usage data.

**Key Takeaway:** PostgreSQL + normalized schema can handle 90% of use cases. Only add Neo4j if deep graph traversal becomes a core user need.

---

**Next Steps:**
1. âœ… Approve this evolution plan
2. âœ… Implement schema normalization (Week 1-2)
3. âœ… Enhance validation & FTS (Week 3-4)
4. âš ï¸ Re-evaluate PostgreSQL migration in Month 3
5. ğŸ”µ Make Neo4j decision in Month 6 based on usage data

---

**Document Version:** 1.0
**Last Updated:** 2025-10-19
**Author:** Database Architecture Specialist
**Status:** Ready for Review & Implementation
